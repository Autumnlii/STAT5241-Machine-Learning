baseLoc = "C:\\Users\\Shuaiwen\\Desktop\\2016 Fall\\Statistical Machine Learning\\Homework 1\\"
histLoc = paste(baseLoc, "histograms.bin", sep="")
H <- matrix(readBin(histLoc, "double", 640000), 40000, 16)
baseLoc = "C:\\Users\\Shuaiwen\\Desktop\\2016 Fall\\Statistical Machine Learning\\Homework 1\\"
histLoc = paste(baseLoc, "histograms.bin", sep="")
H <- matrix(readBin(histLoc, "double", 640000), 40000, 16)
setwd("~/Desktop/2017 spring/GR 5241/HW/hw5")
baseLoc = "~/Desktop/2017 spring/GR 5241/HW/hw5"
histLoc = paste(baseLoc, "histograms.bin", sep="")
H <- matrix(readBin(histLoc, "double", 640000), 40000, 16)
baseLoc = "~/Desktop/2017 spring/GR 5241/HW/hw5"
histLoc = paste(baseLoc, "histograms.bin", sep="")
H <- matrix(readBin(histLoc, "double", 640000), 40000, 16)
histLoc
setwd("~/Desktop/2017 spring/GR 5241/HW/hw5")
data = readBin(histLoc, "double", 640000)
setwd("~/Desktop/2017 spring/GR 5241/HW/hw5")
data = readBin(histLoc, "double", 640000)
?readBin
file = ("histograms.bin")
data = readBin(file, "double", 640000)
data
H = matrix(data,4000,16)
H
n = dim(H)[1]
MultinomialEM <- function(H, K, tau=0.0001){
# H has dimension n * d
# T has dimension K * d
# c has dimension K * 1
# A has dimension n * K
# n = 40000, d = 16, K=3,4,5
# initialization
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
c = rep(1 / K, K)
iter_criterion = c()
A0 = matrix(rep(0, n * K), n, K)
# heat-up E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# heat-up M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
# iteration
while (norm(A - A0, type="1") > tau){
A0 = A
# E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
iter_criterion = c(iter_criterion, norm(A-A0, type="1"))
}
# soft assignments to hard assignments
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_criterion=iter_criterion))  # this return is for more information. This does not satisfy the homework requirement since the only "m" is supposed to be returned
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
K_vec = c(3, 4, 5)
figureLoc = paste(baseLoc, "P3figure_K", K_vec, ".pdf", sep="")
for (i in 1:length(K_vec)){
result = MultinomialEM(H, K=K_vec[i], tau=0.01)
pdf(width=8, height=8, file=figureLoc[i])
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
dev.off()
}
?minusMax
setwd("~/Desktop/2017 spring/GR 5241/HW/hw5")
file = ("histograms.bin")
data = readBin(file, "double", 640000)
H = matrix(data,4000,16)
MultinomialEM <- function(H, K, tau=0.0001){
# H has dimension n * d
# T has dimension K * d
# c has dimension K * 1
# A has dimension n * K
# n = 40000, d = 16, K=3,4,5
# initialization
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
c = rep(1 / K, K)
iter_criterion = c()
A0 = matrix(rep(0, n * K), n, K)
# heat-up E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# heat-up M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
# iteration
while (norm(A - A0, type="1") > tau){
A0 = A
# E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
iter_criterion = c(iter_criterion, norm(A-A0, type="1"))
}
# soft assignments to hard assignments
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_criterion=iter_criterion))  # this return is for more information. This does not satisfy the homework requirement since the only "m" is supposed to be returned
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
K_vec = c(3, 4, 5)
setwd("~/Desktop/2017 spring/GR 5241/HW/hw5")
file = ("histograms.bin")
data = readBin(file, "double", 640000)
H = matrix(data,4000,16)
MultinomialEM <- function(H, K, tau=0.0001){
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
#The centroids t1 , ..., tK . These are Rd vectors
#T has dimension K * d
#Each centroid is the parameter vector of a multinomial distribution and can be regarded as the center of a cluster.
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
#Ck is the cluster weight
#Ck = (num of points in k) / n
c = rep(1 / K, K)
iter_cut = c()
#The entry aik specifies the probability of feature i to be assigned to cluster k.
A0 = matrix(rep(0, n * K), n, K)
# E step formula
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# M step formula
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
# Compute a measure of the change of assignments during the current iteration:
#δ := ||A − Aold||
#Terminate the iteration when δ < τ
while (norm(A - A_old, type="1") > tau){
A_old = A
# E step
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# M step
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
iter_cut = c(iter_cut, norm(A-A0, type="1"))
}
# Turn the soft assignments into a vector m of hard assignments
# mi := arg max aik
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_cut=iter_cut))
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
normalize
normalize <- function(v) v / sum(v)
v / sum(v)
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
K_vec = c(3, 4, 5)
result = MultinomialEM(H, K=K_vec[i], tau=0.01)
result = MultinomialEM(H, K=K_vec[i], tau=0.01)
setwd("~/Desktop/2017 spring/GR 5241/HW/hw5")
file = ("histograms.bin")
data = readBin(file, "double", 640000)
H = matrix(data,4000,16)
MultinomialEM <- function(H, K, tau=0.0001){
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
#The centroids t1 , ..., tK . These are Rd vectors
#T has dimension K * d
#Each centroid is the parameter vector of a multinomial distribution and can be regarded as the center of a cluster.
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
#Ck is the cluster weight
#Ck = (num of points in k) / n
c = rep(1 / K, K)
iter_cut = c()
#The entry aik specifies the probability of feature i to be assigned to cluster k.
A0 = matrix(rep(0, n * K), n, K)
# E step formula
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# M step formula
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
# Compute a measure of the change of assignments during the current iteration:
#δ := ||A − Aold||
#Terminate the iteration when δ < τ
while (norm(A - A_old, type="1") > tau){
A_old = A
# E step
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(theta, 1, normalize))
# M step
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
iter_cut = c(iter_cut, norm(A-A0, type="1"))
}
# Turn the soft assignments into a vector m of hard assignments
# mi := arg max aik
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_cut=iter_cut))
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
K_vec = c(3, 4, 5)
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
MultinomialEM <- function(H, K, tau=0.0001){
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
#The centroids t1 , ..., tK . These are Rd vectors
#T has dimension K * d
#Each centroid is the parameter vector of a multinomial distribution and can be regarded as the center of a cluster.
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
#Ck is the cluster weight
#Ck = (num of points in k) / n
c = rep(1 / K, K)
iter_cut = c()
#The entry aik specifies the probability of feature i to be assigned to cluster k.
A0 = matrix(rep(0, n * K), n, K)
# E step formula
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(theta, 1, normalize))
# M step formula
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
# Compute a measure of the change of assignments during the current iteration:
#δ := ||A − Aold||
#Terminate the iteration when δ < τ
while (norm(A - A_old, type="1") > tau){
A_old = A
# E step
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(theta, 1, normalize))
# M step
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
iter_cut = c(iter_cut, norm(A-A0, type="1"))
}
# Turn the soft assignments into a vector m of hard assignments
# mi := arg max aik
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_cut=iter_cut))
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
MultinomialEM <- function(H, K, tau=0.0001){
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
#The centroids t1 , ..., tK . These are Rd vectors
#T has dimension K * d
#Each centroid is the parameter vector of a multinomial distribution and can be regarded as the center of a cluster.
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
#Ck is the cluster weight
#Ck = (num of points in k) / n
c = rep(1 / K, K)
iter_cut = c()
#The entry aik specifies the probability of feature i to be assigned to cluster k.
A0 = matrix(rep(0, n * K), n, K)
# E step formula
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(theta, 1, normalize))
# M step formula
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
# Compute a measure of the change of assignments during the current iteration:
#δ := ||A − Aold||
#Terminate the iteration when δ < τ
while (norm(A - A0, type="1") > tau){
A0 = A
# E step
theta_1 = t(apply(tcrossprod(H, log(T)), 1, minusMax))
theta = exp(theta_1) %*% diag(c)
A = t(apply(theta, 1, normalize))
# M step
c = colSums(A) / n
bk = crossprod(A, H)
tk = diag(1 / rowSums(bk)) %*% bk
iter_cut = c(iter_cut, norm(A-A0, type="1"))
}
# Turn the soft assignments into a vector m of hard assignments
# mi := arg max aik
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_cut=iter_cut))
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
K_vec = c(3, 4, 5)
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
plot(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
plot(matrix(result$m, 200, 200, byrow=T))
matrix(result$m, 200, 200, byrow=T)
plot(matrix(result$m, 200, 200, byrow=T))
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
dev.off()
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
pdf(width=8, height=8, file=figureLoc[1])
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
plot(1:length(result$iter_cut), result$iter_cut, type="o", xlab="#iterations", ylab="||A-A0||", main="K=5, tau=0.01")
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
pdf(width=8, height=8, file=figureLoc[1])
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
baseLoc = "~/Desktop/2017 spring/GR 5241/HW/hw5/"
histLoc = paste(baseLoc, "histograms.bin", sep="")
H <- matrix(readBin(histLoc, "double", 640000), 40000, 16)
MultinomialEM <- function(H, K, tau=0.0001){
# H has dimension n * d
# T has dimension K * d
# c has dimension K * 1
# A has dimension n * K
# n = 40000, d = 16, K=3,4,5
# initialization
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
c = rep(1 / K, K)
iter_criterion = c()
A0 = matrix(rep(0, n * K), n, K)
# heat-up E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# heat-up M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
# iteration
while (norm(A - A0, type="1") > tau){
A0 = A
# E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
iter_criterion = c(iter_criterion, norm(A-A0, type="1"))
}
# soft assignments to hard assignments
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_criterion=iter_criterion))  # this return is for more information. This does not satisfy the homework requirement since the only "m" is supposed to be returned
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
K_vec = c(3, 4, 5)
figureLoc = paste(baseLoc, "P3figure_K", K_vec, ".pdf", sep="")
for (i in 1:length(K_vec)){
result = MultinomialEM(H, K=K_vec[i], tau=0.01)
pdf(width=8, height=8, file=figureLoc[i])
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
dev.off()
}
pdf(width=6, height=6, file=paste(baseLoc, "norm_vs_iter.pdf", sep=""))
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="#iterations", ylab="||A-A0||", main="K=5, tau=0.01")
dev.off()
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="#iterations", ylab="||A-A0||", main="K=5, tau=0.01")
K_vec = c(3, 4, 5)
figureLoc = paste(baseLoc, "P3figure_K", K_vec, ".pdf", sep="")
for (i in 1:length(K_vec)){
result = MultinomialEM(H, K=K_vec[i], tau=0.01)
pdf(width=8, height=8, file=figureLoc[i])
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
dev.off()
}
pdf(width=6, height=6, file=paste(baseLoc, "norm_vs_iter.pdf", sep=""))
setwd("~/Desktop")
setwd("~/Desktop/2017 spring/GR 5241/HW/hw5")
baseLoc = "~/Desktop/2017 spring/GR 5241/HW/hw5/"
histLoc = paste(baseLoc, "histograms.bin", sep="")
H <- matrix(readBin(histLoc, "double", 640000), 40000, 16)
MultinomialEM <- function(H, K, tau=0.0001){
# H has dimension n * d
# T has dimension K * d
# c has dimension K * 1
# A has dimension n * K
# n = 40000, d = 16, K=3,4,5
# initialization
n = dim(H)[1]
d = dim(H)[2]
index = sample(n, K, replace=F)
T = diag(1 / rowSums(H[index, ] + 0.01)) %*% (H[index, ]+0.01)
c = rep(1 / K, K)
iter_criterion = c()
A0 = matrix(rep(0, n * K), n, K)
# heat-up E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# heat-up M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
# iteration
while (norm(A - A0, type="1") > tau){
A0 = A
# E step
A_temp = t(apply(tcrossprod(H, log(T)), 1, minusMax))
A_temp = exp(A_temp) %*% diag(c)
A = t(apply(A_temp, 1, normalize))
# M step
T_temp = crossprod(A, H)
T      = diag(1 / rowSums(T_temp)) %*% T_temp
c = colSums(A) / n
iter_criterion = c(iter_criterion, norm(A-A0, type="1"))
}
# soft assignments to hard assignments
m = apply(A, 1, which.max)
return(list(m=m, A=A, T=T, c=c, iter_criterion=iter_criterion))  # this return is for more information. This does not satisfy the homework requirement since the only "m" is supposed to be returned
}
normalize <- function(v) v / sum(v)
minusMax <- function(v) v - max(v)
K_vec = c(3, 4, 5)
figureLoc = paste(baseLoc, "P3figure_K", K_vec, ".pdf", sep="")
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[1], sep=""))
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[1], sep=""))
result = MultinomialEM(H, K=K_vec[1], tau=0.01)
result$m
plot(reault$m)
plot(result$m)
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[1], sep=""))
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="#iterations", ylab="||A-A0||", main="K=5, tau=0.01")
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="#iterations", ylab="||A-A0||", main="K=5, tau=0.01")
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="#iterations", ylab="||A-A0||", main="K=5, tau=0.01")
K_vec = c(3, 4, 5)
figureLoc = paste(baseLoc, "P3figure_K", K_vec, ".pdf", sep="")
for (i in 1:length(K_vec)){
result = MultinomialEM(H, K=K_vec[i], tau=0.01)
pdf(width=8, height=8, file=figureLoc[i])
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
dev.off()
}
pdf(width=6, height=6, file=paste(baseLoc, "norm_vs_iter.pdf", sep=""))
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="#iterations", ylab="||A-A0||", main="K=5, tau=0.01")
dev.off()
pdf(width=6, height=6, file=paste(baseLoc, "norm_vs_iter.pdf", sep=""))
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="number of iterations", ylab="||A-A0||", main="K=5, tau=0.01")
pdf(width=6, height=6, file=paste(baseLoc, "norm_vs_iter.pdf", sep=""))
plot(1:length(result$iter_criterion), result$iter_criterion, type="o", xlab="number of iterations", ylab="||A-A0||", main="K=5, tau=0.01")
dev.off()
K_vec = c(3, 4, 5)
figureLoc = paste(baseLoc, "P3figure_K", K_vec, ".pdf", sep="")
for (i in 1:length(K_vec)){
result = MultinomialEM(H, K=K_vec[i], tau=0.01)
pdf(width=8, height=8, file=figureLoc[i])
image(matrix(result$m, 200, 200, byrow=T), col=gray((0:16)/16), main=paste("K=", K_vec[i], sep=""))
dev.off()
}
par(mfrow = c(1:3))
par(mfrow = c(1,3))
